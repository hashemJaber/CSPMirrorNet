{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are coding CSPDarkNet,  FPN, PAN, SPP, RPN, and Detection Heads with some of our model tweaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn((1, 3, 350, 350))  # Example feature map from base CNN, or an image even\n",
    "input_layer= InputLayer(250,250)\n",
    "print(images.shape)\n",
    "\n",
    "images=input_layer(images,True)\n",
    "\n",
    "out_put_conv_layer=ConvBlockCSP(images.shape[1],4)(images)\n",
    "\n",
    "item1,item2=out_put_conv_layer\n",
    "print(item1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##Testing BackBone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_back_bone=BaseConvBlockCSP(images.shape[1],4)\n",
    "out_put_conv_layer=model_back_bone(images,use_single_layer=False)\n",
    "item1,item2=out_put_conv_layer\n",
    "print(item1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer(nn.Module):\n",
    "   def __init__(self,target_height,target_width)->None:\n",
    "      super(InputLayer,self).__init__()\n",
    "      self.target_height = target_height\n",
    "      self.target_width = target_width\n",
    "      self.resize_transform = transforms.Resize((target_height, target_width))\n",
    "\n",
    "\n",
    "   def forward(self, images:Tensor,transform_to_fixed:bool=False)-> Tensor:\n",
    "       \"\"\"\n",
    "       input:\n",
    "          transform_to_fixed: bool #True: to resize images not matching accepted fixed lenght, False: to use default images size\n",
    "          images: Tensor of shape (B,C,W,H) #Batch size, Channel, Width, and Height\n",
    "       output:\n",
    "          images: Tensor of shape (B, C, W2,H2) #Batch size, Channel, Width (updated to accept fixed lenght), and Height(updated to accept fixed length)\n",
    "\n",
    "       Note: transform_to_fixed default to False due to us adding the SPP layer later on, SPP will enbale to use most of the networks upto the head detection layer's neck with any size of image's width's and heights\n",
    "       \"\"\"\n",
    "       if(transform_to_fixed):\n",
    "          # Convert the tensor to PIL Image, apply the resize, and convert back to a tensor\n",
    "          image = F.interpolate(images, size=(self.target_height, self.target_width), mode='bilinear', align_corners=False)\n",
    "          return image\n",
    "\n",
    "       return image\n",
    "\n",
    "\n",
    "# Question: Should we sum after applying activations or before ?, then concatinate ?\n",
    "# WOuld not change model performances, does not add any feaure map richness/complexity\n",
    "class ConvBlockCSP(nn.Module):\n",
    "   \"\"\"\n",
    "   ConvBlockCSP: Similar to CSPNet except in the layer we concatinate we add its values to the layer after it, and then concnatinate both of them.\n",
    "   input:\n",
    "      input_channel: int, number of channels to input\n",
    "      out_put_channel: int, number of channels to output\n",
    "      kernet_size: tuple (w,h), default: (1,1)\n",
    "      stride:int, effect the (W-kenrnel_size[0]+2*padding)/stride\n",
    "   output:\n",
    "         original feature map, new feature map \n",
    "   \"\"\"\n",
    "   def __init__(self, input_channel: int,out_put_channel:int,stride:int=2, kernel_size=(5,5))->None:\n",
    "       super(ConvBlockCSP,self).__init__()\n",
    "       self.conv1= nn.Conv2d(input_channel,out_put_channel,kernel_size=kernel_size,stride=2,padding=1 )\n",
    "       self.activation = nn.Mish()\n",
    "\n",
    "       self.conv2= nn.Conv2d(out_put_channel,out_put_channel,kernel_size=kernel_size,stride=1 )\n",
    "       self.activation2 = nn.Mish()\n",
    "\n",
    "       self.bn = nn.BatchNorm2d(out_put_channel)\n",
    "\n",
    "       self.bn2 = nn.BatchNorm2d(out_put_channel)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "       #TODO: Add batch normalization\n",
    "\n",
    "\n",
    "   def forward(self, feauture_map) -> (Tensor, Tensor):\n",
    "     x1 = self.bn(self.conv1(feauture_map))\n",
    "     x1=self.activation(x1)\n",
    "\n",
    "     x2 = self.bn(self.conv2(x1))\n",
    "     x2=self.activation(x2)\n",
    "     #in case we change kernel size to !=(1,1), x1 needs to match 2 for summation, Fixed\n",
    "     if x1.size(2) != x2.size(2) or x1.size(3) != x2.size(3):\n",
    "            x1 = nn.functional.adaptive_avg_pool2d(x1, (x2.size(2), x2.size(3)))\n",
    "\n",
    "\n",
    "\n",
    "     x3 = torch.cat([x1,x2+x1],1)\n",
    "     print('x3 shape ',x3.shape)\n",
    "     return (feauture_map,x3 )\n",
    "\n",
    "#Take Part 1 and Part 2, then switch to the other parts, such as part 2, and part 1 for cspNet\n",
    "\n",
    "# Idea: We generate a Parameter generating network to automaically solve for hyper parameters, we use it every 100 epochs as a loss, or maybe the lowest of 10 of all the 100 epochs\n",
    "class BaseConvBlockCSP(nn.Module):\n",
    "  def __init__(self,input_channel: int,out_put_channel:int)->None:\n",
    "\n",
    "    super(BaseConvBlockCSP,self).__init__()\n",
    "    self.ConvBlockCSP= ConvBlockCSP(input_channel,out_put_channel) # In=3, out=8\n",
    "    self.ConvBlockCSP2= ConvBlockCSP(out_put_channel*2,out_put_channel*4) #in= 8 out_put_channel*2, out=32\n",
    "    self.ConvBlockCSP3= ConvBlockCSP(32,out_put_channel*6)#in 32, out= 48\n",
    "\n",
    "    self.channel_adjust = nn.Conv2d(3, out_put_channel*6, kernel_size=1, stride=1)\n",
    "\n",
    "        #TODO, Replace with a forloop initialization\n",
    "\n",
    "  def forward(self, base_feature_map:Tensor, use_single_layer:bool=True)->Tensor:\n",
    "    x1=self.ConvBlockCSP(base_feature_map)\n",
    "\n",
    "    if(use_single_layer):\n",
    "          \n",
    "        return x1\n",
    "    x1_part1,x1_part2=x1\n",
    "    print('x1 part 1',x1_part1.shape)\n",
    "    print('x1 part 2',x1_part2.shape)\n",
    "    x2_part1,x2_part2=self.ConvBlockCSP2(x1_part2) \n",
    "\n",
    "    print('x2 part 1',x2_part1.shape)\n",
    "    print('x2 part 2',x2_part2.shape) \n",
    "\n",
    "    x3_part1, x3_part2=self.ConvBlockCSP3(x2_part2)\n",
    "    print('x3 part 1',x3_part1.shape)\n",
    "    print('x3 part 2',x3_part2.shape) \n",
    "    x1_part1=self.channel_adjust(x1_part1)\n",
    "    print('x1_part1 new shape',x1_part1.shape)\n",
    "    \n",
    "    x1_part1 = nn.functional.adaptive_avg_pool2d(x1_part1, (x3_part2.size(2), x3_part2.size(3)))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    return (x3_part1, torch.cat([x1_part1,x3_part2],1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CSPDarkNet(nn.Module):\n",
    "   \"\"\"\n",
    "   CSPDarkNet:\n",
    "    unlike the original CSPNet, where we would only take 1 part to pass through convolutions while \n",
    "    skipping the untoched part to join the output of those convultions, we will be doing the same \n",
    "    in this architecture with the adddition that we will be joining both both parts in the same\n",
    "    operation, same model, two different outputs, and concatinate the 2 branches outputs. Hoping to increase the richness of the feature represenation.\n",
    "\n",
    "    input: \n",
    "        image: Tensor\n",
    "\n",
    "    outout:\n",
    "        Feaute Map:Tensor\n",
    "   \"\"\"\n",
    "\n",
    "   def __init__(self,num_of_base_blocks:int,input_shape:Tensor)->None:\n",
    "      super(CSPDarkNet,self).__init__()\n",
    "      self.base_blocks = nn.ModuleList()\n",
    "      for i in range(num_of_base_blocks):\n",
    "        self.base_blocks.append(BaseConvBlockCSP(input_shape,4)) #TODO FIX/CHANGEME\n",
    "      self.base_block = BaseConvBlockCSP(input_shape,4)\n",
    "\n",
    "\n",
    "   def forward(self, feature_map:Tensor)->Tensor: #also can be an image/Feaute map\n",
    "      print('before : example size',feature_map.shape )\n",
    "      \n",
    "      example1,example2 = torch.split(feature_map, 2,)\n",
    "      example1 = np.array(example1)\n",
    "\n",
    "      print('example size',example1.shape )\n",
    "\n",
    "      \n",
    "      return example1\n",
    "\n",
    "\n",
    "\n",
    "class FPN(nn.Module):\n",
    "  \"\"\"\n",
    "  Feature Pyramid Network\n",
    "  \n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(FPN,self).__init__()\n",
    "  def forward(self)->Tensor:\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "class PAN(nn.Module):\n",
    "  \"\"\"\n",
    "  Path Aggregation Network\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(PAN,self).__init__()\n",
    "  def forward()->Tensor:\n",
    "    return None\n",
    "\n",
    "\n",
    "class APANFPN():\n",
    "  \"Attention Path Aggregation Network Feature Pyramid Netork\"\n",
    "  def _init__(self):\n",
    "    super(APANFPN,self).__init__()\n",
    "  def forward(self)->None:\n",
    "    return None  \n",
    "\n",
    "\n",
    "class RPN(nn.Module):\n",
    "  \"\"\"\n",
    "  Region Proposal Network\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(RPN,self).__init__()\n",
    "  def forward()->None:\n",
    "    return None\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

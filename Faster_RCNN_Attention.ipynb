{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are coding CSPDarkNet,  FPN, PAN, SPP, RPN, and Detection Heads with some of our model tweaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal 1: \n",
    "We propose that using avg poolings, permutes, as well as channel manupilation and then summations/concatinations can increase feature size representations while not adding computational expensiveness. This in mind said that the closer we are to the origin of the gradients, the less information we loose due to averaging decay, unless a matching/close dimention shape is added in the inermediate layers, such that the model architectuctres mimicks that of the U-Net Network.\n",
    "### Proposal 2:\n",
    "We propose that horizentally increasing the feature map dimentions will help and assist in increasing gradient representation via using a siamese network for both parts of CSPNet, which will require less computations while also increasing feature representations. \n",
    "### Proposal 3:\n",
    "We Propose that cross section percentage feature map sharing for the CSPNet which will help and asssit in feauter represeantion in maintaining some edge information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn((1, 3, 350, 350))  # Example feature map from base CNN, or an image even\n",
    "input_layer= InputLayer(250,250)\n",
    "print(images.shape)\n",
    "\n",
    "images=input_layer(images,True)\n",
    "\n",
    "out_put_conv_layer=ConvBlockCSP(images.shape[1],4)(images)\n",
    "\n",
    "item1,item2=out_put_conv_layer\n",
    "print(item1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Testing BackBone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_back_bone=BaseConvBlockCSP(images.shape[1],4)\n",
    "out_put_conv_layer=model_back_bone(images,use_single_layer=False)\n",
    "item1,item2=out_put_conv_layer\n",
    "print(item1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE TO SELF: USE SPARSE CONVOLUITONS IN EARLIER LAYERS ?????\n",
    "class InputLayer(nn.Module):\n",
    "   def __init__(self,target_height,target_width)->None:\n",
    "      super(InputLayer,self).__init__()\n",
    "      self.target_height = target_height\n",
    "      self.target_width = target_width\n",
    "      self.resize_transform = transforms.Resize((target_height, target_width))\n",
    "\n",
    "\n",
    "   def forward(self, images:Tensor,transform_to_fixed:bool=False)-> Tensor:\n",
    "       \"\"\"\n",
    "       input:\n",
    "          transform_to_fixed: bool #True: to resize images not matching accepted fixed lenght, False: to use default images size\n",
    "          images: Tensor of shape (B,C,W,H) #Batch size, Channel, Width, and Height\n",
    "       output:\n",
    "          images: Tensor of shape (B, C, W2,H2) #Batch size, Channel, Width (updated to accept fixed lenght), and Height(updated to accept fixed length)\n",
    "\n",
    "       Note: transform_to_fixed default to False due to us adding the SPP layer later on, SPP will enbale to use most of the networks upto the head detection layer's neck with any size of image's width's and heights\n",
    "       \"\"\"\n",
    "       if(transform_to_fixed):\n",
    "          # Convert the tensor to PIL Image, apply the resize, and convert back to a tensor\n",
    "          image = F.interpolate(images, size=(self.target_height, self.target_width), mode='bilinear', align_corners=False)\n",
    "          return image\n",
    "\n",
    "       return image\n",
    "\n",
    "\n",
    "\n",
    "# Question: Should we sum after applying activations or before ?, then concatinate ?\n",
    "# WOuld not change model performances, does not add any feaure map richness/complexity\n",
    "class ConvBlockCSP(nn.Module):\n",
    "   \"\"\"\n",
    "   ConvBlockCSP: Similar to CSPNet except in the layer we concatinate we add its values to the layer after it, and then concnatinate both of them.\n",
    "   input:\n",
    "      input_channel: int, number of channels to input\n",
    "      out_put_channel: int, number of channels to output\n",
    "      kernet_size: tuple (w,h), default: (1,1)\n",
    "      stride:int, effect the (W-kenrnel_size[0]+2*padding)/stride\n",
    "   output:\n",
    "         original feature map, new feature map\n",
    "   \"\"\"\n",
    "   def __init__(self, input_channel: int,out_put_channel:int,stride:int=1, kernel_size:tuple=(2,2), verbose:bool=False)->None:\n",
    "       super(ConvBlockCSP,self).__init__()\n",
    "       self.conv1= nn.Conv2d(input_channel,out_put_channel,kernel_size=kernel_size,stride=stride,padding=1 )\n",
    "       self.activation = nn.Mish()\n",
    "\n",
    "       self.conv2= nn.Conv2d(out_put_channel,out_put_channel,kernel_size=kernel_size,stride=1 )\n",
    "       self.activation2 = nn.Mish()\n",
    "\n",
    "       self.bn = nn.BatchNorm2d(out_put_channel)\n",
    "\n",
    "       self.bn2 = nn.BatchNorm2d(out_put_channel)\n",
    "       self.verbose=verbose\n",
    "\n",
    "       #TODO: Add batch normalization\n",
    "\n",
    "\n",
    "   def forward(self, feauture_map:Tensor) -> (Tensor, Tensor):\n",
    "     x1 = self.bn(self.conv1(feauture_map))\n",
    "     x1=self.activation(x1)\n",
    "\n",
    "     x2 = self.bn2(self.conv2(x1))\n",
    "     x2=self.activation(x2)\n",
    "     #in case we change kernel size to !=(1,1), x1 needs to match 2 for summation, Fixed\n",
    "     if x1.size(2) != x2.size(2) or x1.size(3) != x2.size(3):\n",
    "            x1 = nn.functional.adaptive_avg_pool2d(x1, (x2.size(2), x2.size(3)))\n",
    "\n",
    "     x3 = torch.cat([x1,x2+x1],1)\n",
    "\n",
    "     if(self.verbose):\n",
    "       print('x3 shape ',x3.shape)\n",
    "     return (feauture_map, x3 )\n",
    "\n",
    "#Take Part 1 and Part 2, then switch to the other parts, such as part 2, and part 1 for cspNet\n",
    "\n",
    "# Idea: We generate a Parameter generating network to automaically solve for hyper parameters, we use it every 100 epochs as a loss, or maybe the lowest of 10 of all the 100 epochs\n",
    "class BaseConvBlockCSP(nn.Module):\n",
    "  \"\"\"\n",
    "  BaseConvBlockCSP: Will be fixed soon\n",
    "   input:\n",
    "         Feautrue Map: Tensor\n",
    "\n",
    "   output:\n",
    "         tuple: (Tensor,Tensor)\n",
    "  \"\"\"\n",
    "  def __init__(self,input_channel: int,out_put_channel:int,num_blocks:int=3,verbose:bool=False,use_single_layer:bool=True,stride:int=1)->None:\n",
    "\n",
    "    super(BaseConvBlockCSP,self).__init__()\n",
    "    self.verbose=verbose\n",
    "\n",
    "    self.ConvBlockCSP= ConvBlockCSP(input_channel,out_put_channel) # In=3, out=8\n",
    "    ###Left for debugging purposes\n",
    "    self.ConvBlockCSP2= ConvBlockCSP(out_put_channel*2,out_put_channel*4,stride=stride) #in= 8 out_put_channel*2, out=32\n",
    "    self.ConvBlockCSP3= ConvBlockCSP(out_put_channel*4,out_put_channel*6,stride=stride)#in 32, out= 48\n",
    "    self.channel_adjust = nn.Conv2d(3, out_put_channel*6, kernel_size=1, stride=1)\n",
    "\n",
    "        #TODO, Replace with a forloop initialization\n",
    "\n",
    "    self.blocks = nn.ModuleList()\n",
    "    self.use_single_layer=use_single_layer\n",
    "    # IF SET TRUE, ERRORS WILL BE THROWN\n",
    "    if not self.use_single_layer:\n",
    "\n",
    "        print('________WARNING_______')\n",
    "        print('This version is not stable')\n",
    "        print('for handling deeper ConvBlockCSP yet')\n",
    "        print('BaseConvBlockCSP is the source of the issue, and will be fixed soon')\n",
    "        print('please set use_single_layer to True')\n",
    "        print('________WARNING_______')\n",
    "\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "\n",
    "                in_channels = input_channel if i == 0 else out_put_channel * (2 * i)\n",
    "                out_channels = out_put_channel * (2 * (i + 1))\n",
    "\n",
    "                self.blocks.append(ConvBlockCSP(in_channels, out_put_channel))\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, base_feature_map:Tensor)->tuple:\n",
    "    x1=self.ConvBlockCSP(base_feature_map)\n",
    "\n",
    "    if(self.use_single_layer):\n",
    "        if(self.verbose):\n",
    "          print('x1 part 1',x1[0].shape)\n",
    "          print('x1 part 2',x1[1].shape)\n",
    "\n",
    "\n",
    "        return x1\n",
    "\n",
    "    x1_part1,x1_part2=x1\n",
    "    if(self.verbose):\n",
    "        print('x1 part 1',x1_part1.shape)\n",
    "        print('x1 part 2',x1_part2.shape)\n",
    "\n",
    "    x2_part1,x2_part2=self.ConvBlockCSP2(x1_part2)\n",
    "    if(self.verbose):\n",
    "        print('x2 part 1',x2_part1.shape)\n",
    "        print('x2 part 2',x2_part2.shape)\n",
    "\n",
    "    x3_part1, x3_part2=self.ConvBlockCSP3(x2_part2)\n",
    "    if(self.verbose):\n",
    "        print('x3 part 1',x3_part1.shape)\n",
    "        print('x3 part 2',x3_part2.shape)\n",
    "    x1_part1=self.channel_adjust(x1_part1)\n",
    "    if(self.verbose):\n",
    "        print('x1_part1 new shape',x1_part1.shape)\n",
    "\n",
    "    x1_part1 = nn.functional.adaptive_avg_pool2d(x1_part1, (x3_part2.size(2), x3_part2.size(3)))\n",
    "\n",
    "\n",
    "    return (x3_part1, torch.cat([x1_part1,x3_part2],1))\n",
    "\n",
    "\n",
    "class CSPMirrorNet(nn.Module):\n",
    "   \"\"\"\n",
    "   CSPDarkNet:\n",
    "    unlike the original CSPNet, where we would only take 1 part to pass through convolutions while\n",
    "    skipping the untoched part to join the output of those convultions, we will be doing the same\n",
    "    in this architecture with the adddition that we will be joining both both parts in the same\n",
    "    operation, same model, two different outputs, and concatinate the 2 branches outputs. Hoping to increase the richness of the feature represenation.\n",
    "\n",
    "    input:\n",
    "        image: Tensor\n",
    "\n",
    "    outout:\n",
    "        Feaute Map:Tensor\n",
    "   \"\"\"\n",
    "\n",
    "   def __init__(self,num_of_base_blocks:int,input_shape:Tensor,verbose:bool=False,overlap_percentage:float=0.20,stride:int=2)->None:\n",
    "      super(CSPMirrorNet,self).__init__()\n",
    "      self.base_blocks = nn.ModuleList()\n",
    "      \"\"\"\n",
    "      for futur iterations, we will enable depth manupilation of the Mished Dense Block, but keeping in mind with every base_block we will conduct resnetlike operations and adaptive pooling, per proposal 1\n",
    "      for i in range(num_of_base_blocks):\n",
    "        self.base_blocks.append(BaseConvBlockCSP(input_shape,4)) #TODO FIX/CHANGEME\n",
    "      \"\"\"\n",
    "      self.base_block = BaseConvBlockCSP(input_shape,4,stride=stride)\n",
    "      self.verbose=verbose\n",
    "      self.overlap_percentage=overlap_percentage\n",
    "\n",
    "\n",
    "   def forward(self, feature_map:Tensor)->Tensor: #also can be an image/Feaute map\n",
    "\n",
    "      if(self.verbose):\n",
    "        print('before : example size',feature_map.shape)\n",
    "\n",
    "\n",
    "      #height_split = feature_map.shape[2] // 2\n",
    "      #width_split = feature_map.shape[3] // 2\n",
    "      #because when even turns to odd, and then we divide, it goes down by one hence it doesn't effect even numbers, this is to resovle the issue of un even odd/even width's and heights when procceses to part1, part2\n",
    "      height= feature_map.shape[2]\n",
    "      width= feature_map.shape[3]\n",
    "      height_split = (height + 1) // 2\n",
    "      width_split = (width + 1) // 2\n",
    "      if(self.verbose):\n",
    "        print('height_split',height_split)\n",
    "        print('width_split',width_split)\n",
    "\n",
    "\n",
    "      height_overlap = int(height_split * self.overlap_percentage)\n",
    "      width_overlap = int(width_split * self.overlap_percentage)\n",
    "\n",
    "      if(self.verbose):\n",
    "        print('height_overlap',height_overlap)\n",
    "        print('width_overlap',width_overlap)\n",
    "\n",
    "      # Using a percentage value for might make the model more flexible by allowing us to scale the overlap relative to the size of the feature map., This is just an idea, not referenced anywhere\n",
    "      part_1 = feature_map[:, :, :height_split + height_overlap, :width_split + width_overlap]\n",
    "      part_2 = feature_map[:, :, height - (height_split + height_overlap):, width - (width_split + width_overlap):]\n",
    "\n",
    "\n",
    "\n",
    "        # Process\n",
    "      if(self.verbose):\n",
    "        print(\"after: \",part_1.shape)\n",
    "        print(\"after: \",part_2.shape)\n",
    "\n",
    "\n",
    "      processed_part1 = self.base_block(part_1)[1]\n",
    "      processed_part2 = self.base_block(part_2)[1]\n",
    "\n",
    "      if(self.verbose):\n",
    "        print(\"processed_part1\", processed_part1.shape)\n",
    "        print(\"processed_part2\", processed_part2.shape)\n",
    "\n",
    "\n",
    "      # Reusing the same logic we did earlier, we are mimicking a CSPNet Part1, Part2 except that we are adding cross sectioned, think of it like a siamese network\n",
    "\n",
    "      example2 = nn.functional.adaptive_avg_pool2d(part_2, (processed_part2.size(2), processed_part2.size(3)))\n",
    "      example1 = nn.functional.adaptive_avg_pool2d(part_1, (processed_part1.size(2), processed_part1.size(3)))\n",
    "\n",
    "      concat1 = torch.cat([example2, processed_part1], dim=1)\n",
    "\n",
    "      concat2 = torch.cat([example1, processed_part2], dim=1)\n",
    "\n",
    "      # Sum the concatenated outputs, we can also opt out to concatinate them but this will increase computational cost\n",
    "      combined_output = concat1 + concat2\n",
    "\n",
    "      if(self.verbose):\n",
    "\n",
    "        print(\"Shape of part1 (untouched):\", example1.shape)\n",
    "        print(\"Shape of example2 (untouched):\", example2.shape)\n",
    "        print(\"Shape of processed_example1:\", processed_part1.shape)\n",
    "        print(\"Shape of processed_example2:\", processed_part2.shape)\n",
    "        print(\"Shape of concat1:\", concat1.shape)\n",
    "        print(\"Shape of concat2:\", concat2.shape)\n",
    "        print(\"Combined output shape after summing:\", combined_output.shape)\n",
    "\n",
    "\n",
    "      return combined_output\n",
    "\n",
    "\n",
    "class FPN(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature Pyramid Network:\n",
    "    input:\n",
    "        Feature map: list, outputs from the backbone\n",
    "\n",
    "    output:\n",
    "          P1,P2,P3 (each P is a payramid)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: list, out_channels: int):\n",
    "        super(FPN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels[0], out_channels, kernel_size=1, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels[1], out_channels, kernel_size=1, stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels[2], out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "        self.upconv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1) #maybe\n",
    "        self.upconv1 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1) #maybe\n",
    "\n",
    "    def forward(self, feature_map: list) -> tuple:\n",
    "\n",
    "        C2, C3, C4 = feature_map\n",
    "\n",
    "        P3 = self.conv3(C4)\n",
    "        P2 = self.conv2(C3) + F.interpolate(P3, scale_factor=2, mode='nearest') #Use Bilinear maybe ?\n",
    "        P1 = self.conv1(C2) + F.interpolate(P2, scale_factor=2, mode='nearest') #Use Bilinear maybe ?\n",
    "\n",
    "        return P1, P2, P3\n",
    "\n",
    "\n",
    "\n",
    "class PAN(nn.Module):\n",
    "  \"\"\"\n",
    "  Path Aggregation Network\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(PAN,self).__init__()\n",
    "    self.conv1= nn.Conv2d(out_put_channel,out_put_channel,kernel_size=kernel_size,stride=1 )\n",
    "  def forward(self, P: list)->Tensor:\n",
    "    P1 , P2, P3 = P\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "class APANFPN():\n",
    "  #We can use permute to reeshape the final layers then padding so we can conduct\n",
    "  \"Attention Path Aggregation Network Feature Pyramid Netork\"\n",
    "  def _init__(self):\n",
    "    super(APANFPN,self).__init__()\n",
    "  def forward(self)->None:\n",
    "    return None\n",
    "\n",
    "\n",
    "class RPN(nn.Module):\n",
    "  \"\"\"\n",
    "  Region Proposal Network\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(RPN,self).__init__()\n",
    "  def forward()->None:\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of the original CSPDenseNet that we will be using for our CSPMirrorNet53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNLeakyReLU(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leaky_relu(self.bn(self.conv(x)))\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, shortcut=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBNLeakyReLU(in_channels, hidden_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv2 = ConvBNLeakyReLU(hidden_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.shortcut = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shortcut:\n",
    "            return x + self.conv2(self.conv1(x))\n",
    "        else:\n",
    "            return self.conv2(self.conv1(x))\n",
    "\n",
    "class CSPBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_bottlenecks, fusion_type=\"last\", partial_transition=True):\n",
    "        \"\"\"\n",
    "        CSPBlock with configurable fusion and partial transition.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_channels = out_channels // 2\n",
    "        self.fusion_type = fusion_type\n",
    "        self.partial_transition = partial_transition\n",
    "\n",
    "        # Two main convolution paths\n",
    "        self.conv1 = ConvBNLeakyReLU(in_channels, hidden_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv2 = ConvBNLeakyReLU(in_channels, hidden_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # Bottleneck sequence\n",
    "        self.bottlenecks = nn.Sequential(\n",
    "            *[BottleneckBlock(hidden_channels, hidden_channels, hidden_channels) for _ in range(num_bottlenecks)]\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv3 = ConvBNLeakyReLU(hidden_channels * 2, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "\n",
    "        if self.partial_transition:\n",
    "        \n",
    "            if self.fusion_type == \"first\":\n",
    "        \n",
    "                x_fused = torch.cat([x1, x2], dim=1)\n",
    "                x1 = self.bottlenecks(x_fused)\n",
    "            else:\n",
    "  \n",
    "                x1 = self.bottlenecks(x1)\n",
    "                x_fused = torch.cat([x1, x2], dim=1)\n",
    "        else:\n",
    "            if self.fusion_type == \"first\":\n",
    "                x_fused = torch.cat([x1, x2], dim=1)\n",
    "                x_fused = self.bottlenecks(x_fused)\n",
    "            else:\n",
    "                x1 = self.bottlenecks(x1)\n",
    "                x_fused = torch.cat([x1, x2], dim=1)\n",
    "\n",
    "        return self.conv3(x_fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSPMirrorNet53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class CSPMirrorNet53(nn.Module):\n",
    "    def __init__(self, num_classes:int=1000, input_shape:int=3, verbose:bool=False, gamma:float=1.0, fusion_type:str=\"last\", partial_transition:bool=True)->None:\n",
    "        \"\"\"\n",
    "        CSPMirrorNet53 with options for fusion type and channel density adjustment.\n",
    "        \n",
    "        Args:\n",
    "            num_classes (int): Number of classes for the final classification.\n",
    "            input_shape (int): Number of input channels (e.g., 3 for RGB).\n",
    "            verbose (bool): Print the shape of each layer if True.\n",
    "            gamma (float): Channel reduction factor to adjust model size (0 < γ ≤ 1).\n",
    "            fusion_type (str): Type of fusion ('first' or 'last') within CSP blocks.\n",
    "            partial_transition (bool): If True, enables partial transition in CSP blocks.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        self.gamma = gamma\n",
    "        self.fusion_type = fusion_type\n",
    "        self.partial_transition = partial_transition\n",
    "        \n",
    "       \n",
    "        adjusted_channels = int(64 * gamma)  # Adjust channels by gamma\n",
    "        self.stem = ConvBNLeakyReLU(input_shape, adjusted_channels, kernel_size=3, stride=1, padding=4)\n",
    "        \n",
    "        # Stage 1\n",
    "        stage1_in = adjusted_channels\n",
    "        stage1_out = int(128 * gamma)\n",
    "        self.stage1 = nn.Sequential(\n",
    "            ConvBNLeakyReLU(stage1_in, stage1_out, kernel_size=3, stride=2, padding=3),\n",
    "            CSPMirrorNet(num_of_base_blocks=1, input_shape=stage1_out, verbose=self.verbose)\n",
    "        )\n",
    "        \n",
    "        # Stage 2\n",
    "        stage2_in = stage1_out + 8  # Adjusted for CSP output concatenation\n",
    "        stage2_out = int(256 * gamma)\n",
    "        self.stage2 = nn.Sequential(\n",
    "            ConvBNLeakyReLU(stage2_in, stage2_out, kernel_size=3, stride=2, padding=1),\n",
    "            CSPMirrorNet(num_of_base_blocks=2, input_shape=stage2_out, verbose=self.verbose)\n",
    "        )\n",
    "        \n",
    "        # Stage 3\n",
    "        stage3_in = stage2_out + 8\n",
    "        stage3_out = int(512 * gamma)\n",
    "        self.stage3 = nn.Sequential(\n",
    "            ConvBNLeakyReLU(stage3_in, stage3_out, kernel_size=3, stride=2, padding=1),\n",
    "            CSPBlock(stage3_out, stage3_out, num_bottlenecks=8, fusion_type=self.fusion_type, partial_transition=self.partial_transition)\n",
    "        )\n",
    "        \n",
    "        # Stage 4\n",
    "        stage4_in = stage3_out\n",
    "        stage4_out = int(1024 * gamma)\n",
    "        self.stage4 = nn.Sequential(\n",
    "            ConvBNLeakyReLU(stage4_in, stage4_out, kernel_size=3, stride=2, padding=2),\n",
    "            CSPMirrorNet(num_of_base_blocks=8, input_shape=stage4_out, verbose=self.verbose)\n",
    "        )\n",
    "        \n",
    "        # Stage 5\n",
    "        stage5_in = stage4_out + 8\n",
    "        stage5_out = int(2048 * gamma)\n",
    "        self.stage5 = nn.Sequential(\n",
    "            ConvBNLeakyReLU(stage5_in, stage5_out, kernel_size=3, stride=1, padding=2),\n",
    "            CSPMirrorNet(num_of_base_blocks=4, input_shape=stage5_out, verbose=self.verbose)\n",
    "        )\n",
    "        \n",
    "        final_channels = stage5_out + 8  # Final output channels after CSPMirrorNet concatenation\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(final_channels, num_classes)\n",
    "\n",
    "    def forward(self, x:Tensor)->Tensor:\n",
    "        if self.verbose:\n",
    "            print(\"Initial input:\", x.shape)\n",
    "            \n",
    "        x = self.stem(x)\n",
    "        if self.verbose:\n",
    "            print(\"After stem:\", x.shape)\n",
    "\n",
    "        x = self.stage1(x)\n",
    "        if self.verbose:\n",
    "            print(\"After stage1:\", x.shape)\n",
    "\n",
    "        x = self.stage2(x)\n",
    "        if self.verbose:\n",
    "            print(\"After stage2:\", x.shape)\n",
    "\n",
    "        x = self.stage3(x)\n",
    "        if self.verbose:\n",
    "            print(\"After stage3:\", x.shape)\n",
    "\n",
    "        x = self.stage4(x)\n",
    "        if self.verbose:\n",
    "            print(\"After stage4:\", x.shape)\n",
    "\n",
    "        x = self.stage5(x)\n",
    "        if self.verbose:\n",
    "            print(\"After stage5:\", x.shape)\n",
    "\n",
    "        x = self.pool(x).view(x.size(0), -1)\n",
    "        if self.verbose:\n",
    "            print(\"After pooling:\", x.shape)\n",
    "\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(42, padding=5),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize CSPMirrorNet53 model\n",
    "model = CSPMirrorNet53(num_classes=10, input_shape=3, verbose=False, gamma=0.8).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), accuracy\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(90):\n",
    "    print(f\"Epoch [{i+1}/{90}]\")\n",
    "    train_loss, train_accuracy = train_one_epoch(model, train_loader, criterion, optimizer, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print('validation')\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

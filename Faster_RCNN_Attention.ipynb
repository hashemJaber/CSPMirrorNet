{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are coding CSPDarkNet,  FPN, PAN, SPP, RPN, and Detection Heads with some of our model tweaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn((1, 3, 350, 350))  # Example feature map from base CNN, or an image even\n",
    "input_layer= InputLayer(250,250)\n",
    "print(images.shape)\n",
    "\n",
    "images=input_layer(images,True)\n",
    "\n",
    "out_put_conv_layer=ConvBlockCSP(images.shape[1],4)(images)\n",
    "\n",
    "item1,item2=out_put_conv_layer\n",
    "print(item1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##Testing BackBone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_back_bone=BaseConvBlockCSP(images.shape[1],4)\n",
    "out_put_conv_layer=model_back_bone(images,use_single_layer=False)\n",
    "item1,item2=out_put_conv_layer\n",
    "print(item1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer(nn.Module):\n",
    "   def __init__(self,target_height,target_width)->None:\n",
    "      super(InputLayer,self).__init__()\n",
    "      self.target_height = target_height\n",
    "      self.target_width = target_width\n",
    "      self.resize_transform = transforms.Resize((target_height, target_width))\n",
    "\n",
    "\n",
    "   def forward(self, images:Tensor,transform_to_fixed:bool=False)-> Tensor:\n",
    "       \"\"\"\n",
    "       input:\n",
    "          transform_to_fixed: bool #True: to resize images not matching accepted fixed lenght, False: to use default images size\n",
    "          images: Tensor of shape (B,C,W,H) #Batch size, Channel, Width, and Height\n",
    "       output:\n",
    "          images: Tensor of shape (B, C, W2,H2) #Batch size, Channel, Width (updated to accept fixed lenght), and Height(updated to accept fixed length)\n",
    "\n",
    "       Note: transform_to_fixed default to False due to us adding the SPP layer later on, SPP will enbale to use most of the networks upto the head detection layer's neck with any size of image's width's and heights\n",
    "       \"\"\"\n",
    "       if(transform_to_fixed):\n",
    "          # Convert the tensor to PIL Image, apply the resize, and convert back to a tensor\n",
    "          image = F.interpolate(images, size=(self.target_height, self.target_width), mode='bilinear', align_corners=False)\n",
    "          return image\n",
    "\n",
    "       return image\n",
    "\n",
    "\n",
    "# Question: Should we sum after applying activations or before ?, then concatinate ?\n",
    "# WOuld not change model performances, does not add any feaure map richness/complexity\n",
    "class ConvBlockCSP(nn.Module):\n",
    "   \"\"\"\n",
    "   ConvBlockCSP: Similar to CSPNet except in the layer we concatinate we add its values to the layer after it, and then concnatinate both of them.\n",
    "   input:\n",
    "      input_channel: int, number of channels to input\n",
    "      out_put_channel: int, number of channels to output\n",
    "      kernet_size: tuple (w,h), default: (1,1)\n",
    "      stride:int, effect the (W-kenrnel_size[0]+2*padding)/stride\n",
    "   output:\n",
    "         original feature map, new feature map\n",
    "   \"\"\"\n",
    "   def __init__(self, input_channel: int,out_put_channel:int,stride:int=2, kernel_size:tuple=(5,5))->None:\n",
    "       super(ConvBlockCSP,self).__init__()\n",
    "       self.conv1= nn.Conv2d(input_channel,out_put_channel,kernel_size=kernel_size,stride=2,padding=1 )\n",
    "       self.activation = nn.Mish()\n",
    "\n",
    "       self.conv2= nn.Conv2d(out_put_channel,out_put_channel,kernel_size=kernel_size,stride=1 )\n",
    "       self.activation2 = nn.Mish()\n",
    "\n",
    "       self.bn = nn.BatchNorm2d(out_put_channel)\n",
    "\n",
    "       self.bn2 = nn.BatchNorm2d(out_put_channel)\n",
    "\n",
    "       #TODO: Add batch normalization\n",
    "\n",
    "\n",
    "   def forward(self, feauture_map) -> (Tensor, Tensor):\n",
    "     x1 = self.bn(self.conv1(feauture_map))\n",
    "     x1=self.activation(x1)\n",
    "\n",
    "     x2 = self.bn(self.conv2(x1))\n",
    "     x2=self.activation(x2)\n",
    "     #in case we change kernel size to !=(1,1), x1 needs to match 2 for summation, Fixed\n",
    "     if x1.size(2) != x2.size(2) or x1.size(3) != x2.size(3):\n",
    "            x1 = nn.functional.adaptive_avg_pool2d(x1, (x2.size(2), x2.size(3)))\n",
    "\n",
    "\n",
    "\n",
    "     x3 = torch.cat([x1,x2+x1],1)\n",
    "     print('x3 shape ',x3.shape)\n",
    "     return (feauture_map, x3 )\n",
    "\n",
    "#Take Part 1 and Part 2, then switch to the other parts, such as part 2, and part 1 for cspNet\n",
    "\n",
    "# Idea: We generate a Parameter generating network to automaically solve for hyper parameters, we use it every 100 epochs as a loss, or maybe the lowest of 10 of all the 100 epochs\n",
    "class BaseConvBlockCSP(nn.Module):\n",
    "  \"\"\"\n",
    "  BaseConvBlockCSP:\n",
    "   input:\n",
    "\n",
    "   output:        \n",
    "  \"\"\"\n",
    "  def __init__(self,input_channel: int,out_put_channel:int,num_blocks:int=3)->None:\n",
    "\n",
    "    super(BaseConvBlockCSP,self).__init__()\n",
    "    self.ConvBlockCSP= ConvBlockCSP(input_channel,out_put_channel) # In=3, out=8\n",
    "    self.ConvBlockCSP2= ConvBlockCSP(out_put_channel*2,out_put_channel*4) #in= 8 out_put_channel*2, out=32\n",
    "    self.ConvBlockCSP3= ConvBlockCSP(out_put_channel*4,out_put_channel*6)#in 32, out= 48\n",
    "\n",
    "\n",
    "    self.channel_adjust = nn.Conv2d(3, out_put_channel*6, kernel_size=1, stride=1)\n",
    "\n",
    "        #TODO, Replace with a forloop initialization\n",
    "\n",
    "    self.blocks = nn.ModuleList()\n",
    "\n",
    "\n",
    "    for i in range(num_blocks):\n",
    "            \n",
    "            in_channels = input_channel if i == 0 else out_put_channel * (2 * i)\n",
    "            out_channels = out_put_channel * (2 * (i + 1))\n",
    "            \n",
    "         \n",
    "            self.blocks.append(ConvBlockCSP(in_channels, out_put_channel))\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, base_feature_map:Tensor, use_single_layer:bool=True)->Tensor:\n",
    "    x1=self.ConvBlockCSP(base_feature_map)\n",
    "\n",
    "    if(use_single_layer):\n",
    "\n",
    "        return x1\n",
    "\n",
    "\n",
    "\n",
    "    x1_part1,x1_part2=x1\n",
    "    print('x1 part 1',x1_part1.shape)\n",
    "    print('x1 part 2',x1_part2.shape)\n",
    "    x2_part1,x2_part2=self.ConvBlockCSP2(x1_part2)\n",
    "\n",
    "    print('x2 part 1',x2_part1.shape)\n",
    "    print('x2 part 2',x2_part2.shape)\n",
    "\n",
    "    x3_part1, x3_part2=self.ConvBlockCSP3(x2_part2)\n",
    "    print('x3 part 1',x3_part1.shape)\n",
    "    print('x3 part 2',x3_part2.shape)\n",
    "    x1_part1=self.channel_adjust(x1_part1)\n",
    "    print('x1_part1 new shape',x1_part1.shape)\n",
    "\n",
    "    x1_part1 = nn.functional.adaptive_avg_pool2d(x1_part1, (x3_part2.size(2), x3_part2.size(3)))\n",
    "\n",
    "\n",
    "    return (x3_part1, torch.cat([x1_part1,x3_part2],1))\n",
    "\n",
    "\n",
    "class CSPDarkNet(nn.Module):\n",
    "   \"\"\"\n",
    "   CSPDarkNet:\n",
    "    unlike the original CSPNet, where we would only take 1 part to pass through convolutions while\n",
    "    skipping the untoched part to join the output of those convultions, we will be doing the same\n",
    "    in this architecture with the adddition that we will be joining both both parts in the same\n",
    "    operation, same model, two different outputs, and concatinate the 2 branches outputs. Hoping to increase the richness of the feature represenation.\n",
    "\n",
    "    input:\n",
    "        image: Tensor\n",
    "\n",
    "    outout:\n",
    "        Feaute Map:Tensor\n",
    "   \"\"\"\n",
    "\n",
    "   def __init__(self,num_of_base_blocks:int,input_shape:Tensor)->None:\n",
    "      super(CSPDarkNet,self).__init__()\n",
    "      self.base_blocks = nn.ModuleList()\n",
    "      for i in range(num_of_base_blocks):\n",
    "        self.base_blocks.append(BaseConvBlockCSP(input_shape,4)) #TODO FIX/CHANGEME\n",
    "      self.base_block = BaseConvBlockCSP(input_shape,4)\n",
    "\n",
    "\n",
    "   def forward(self, feature_map:Tensor)->Tensor: #also can be an image/Feaute map\n",
    "      print('before : example size',feature_map.shape )\n",
    "\n",
    "      height_split = feature_map.shape[2] // 2\n",
    "      width_split = feature_map.shape[3] // 2\n",
    "      overlap_percentage=0.20 #could be changed\n",
    "\n",
    "\n",
    "      height_overlap = int(height_split * overlap_percentage)\n",
    "      width_overlap = int(width_split * overlap_percentage)\n",
    "\n",
    "      # Using a percentage value for might make the model more flexible by allowing us to scale the overlap relative to the size of the feature map., This is just an idea, not referenced anywhere\n",
    "      example1 = feature_map[:, :, :height_split + height_overlap, :width_split + width_overlap]\n",
    "      example2 = feature_map[:, :, height_split - height_overlap:, width_split - width_overlap:]\n",
    "\n",
    "        # Process\n",
    "      print(\"after: \",example1.shape)\n",
    "      print(\"after: \",example2.shape)\n",
    "\n",
    "\n",
    "      processed_example1 = self.base_block(example1)[1]\n",
    "      processed_example2 = self.base_block(example2)[1]\n",
    "      print(\"processed_example1\", processed_example1.shape)\n",
    "\n",
    "      # Reusing the same logic we did earlier, we are mimicking a CSPNet Part1, Part2 except that we are adding cross sectioned, think of it like a siamese network\n",
    "\n",
    "      example2 = nn.functional.adaptive_avg_pool2d(example2, (processed_example1.size(2), processed_example1.size(3)))\n",
    "      example1 = nn.functional.adaptive_avg_pool2d(example1, (processed_example1.size(2), processed_example1.size(3)))\n",
    "\n",
    "      concat1 = torch.cat([example2, processed_example1], dim=1)\n",
    "\n",
    "      concat2 = torch.cat([example1, processed_example2], dim=1)\n",
    "\n",
    "      # Sum the concatenated outputs, we can also opt out to concatinate them but this will increase computational cost \n",
    "      combined_output = concat1 + concat2\n",
    "\n",
    "      print(\"Shape of example1 (untouched):\", example1.shape)\n",
    "      print(\"Shape of example2 (untouched):\", example2.shape)\n",
    "      print(\"Shape of processed_example1:\", processed_example1.shape)\n",
    "      print(\"Shape of processed_example2:\", processed_example2.shape)\n",
    "      print(\"Shape of concat1:\", concat1.shape)\n",
    "      print(\"Shape of concat2:\", concat2.shape)\n",
    "      print(\"Combined output shape after summing:\", combined_output.shape)\n",
    "\n",
    "\n",
    "      return combined_output\n",
    "\n",
    "\n",
    "\n",
    "class FPN(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature Pyramid Network:\n",
    "    input:\n",
    "        Feature map: list, outputs from the backbone\n",
    "\n",
    "    output:\n",
    "          P1,P2,P3 (each P is a payramid)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: list, out_channels: int):\n",
    "        super(FPN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels[0], out_channels, kernel_size=1, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels[1], out_channels, kernel_size=1, stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels[2], out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "        self.upconv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1) #maybe\n",
    "        self.upconv1 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1) #maybe\n",
    "\n",
    "    def forward(self, feature_map: list) -> tuple:\n",
    "\n",
    "        C2, C3, C4 = feature_map\n",
    "\n",
    "        P3 = self.conv3(C4)\n",
    "        P2 = self.conv2(C3) + F.interpolate(P3, scale_factor=2, mode='nearest') #Use Bilinear maybe ?\n",
    "        P1 = self.conv1(C2) + F.interpolate(P2, scale_factor=2, mode='nearest') #Use Bilinear maybe ?\n",
    "\n",
    "        return P1, P2, P3\n",
    "\n",
    "\n",
    "class PAN(nn.Module):\n",
    "  \"\"\"\n",
    "  Path Aggregation Network\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(PAN,self).__init__()\n",
    "    self.conv1= nn.Conv2d(out_put_channel,out_put_channel,kernel_size=kernel_size,stride=1 )\n",
    "  def forward(self, P: list)->Tensor:\n",
    "    P1 , P2, P3 = P\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "class APANFPN():\n",
    "  \"Attention Path Aggregation Network Feature Pyramid Netork\"\n",
    "  def _init__(self):\n",
    "    super(APANFPN,self).__init__()\n",
    "  def forward(self)->None:\n",
    "    return None\n",
    "\n",
    "\n",
    "class RPN(nn.Module):\n",
    "  \"\"\"\n",
    "  Region Proposal Network\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(RPN,self).__init__()\n",
    "  def forward()->None:\n",
    "    return None\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
